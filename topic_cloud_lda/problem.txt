Lab 2 Predicting Yesterday's Stock Price:
Objective:

The objective of this lab is to create a streaming data pipeline using Apache Spark and Apache Kafka in which future stock prices are predicted based on historical data. Your goal is to get the "plumbing" correct – not to accurately predict a stock price!

Data:

We will be using historical financial data from Yahoo! Finance. You can work with whichever stocks you want for the purpose of developing and testing your lab. In order to get all historical daily stock data for Apple from 2012 to present, for example, you type the following command in a terminal window on your sandbox:

 

$ wget http://ichart.yahoo.com/table.csv\?s=AAPL\&a=0\&b=1\&c=2012\&d=11\&e=31\&f=2017

 

The file generated has the following schema:

Date,Open,High,Low,Close,Volume,Adj Close

 

Note that the data is provided from most to least recent, so you will need to reverse sort the data in order to simulate our "real-time" pipeline.  Below command works on our linux VM:

 

$ sed -n '1!G;h;$p' <input-file> > <output-file>

 
Standalone Kafka Producer:

Implement the TODO's in the StockProducer.java source code provided for this lab. The JSON producer record must conform to the following sample:

 

{

"timestamp":"2012-01-30",

"open":28.190001,

"high":28.690001,

"low":28.02,

"close":28.6,

"volume":23294900

}

The syntax for running the standalone Java Kafka producer is given below:

java -cp CS185-jar-with-dependencies.jar Lab2.StockProducer localhost:9092 DATA/orcl.csv \

orcl prices 1000

 

Spark Streaming Application

Implement the TODO's in the StockSparkApp.java source code provided for this lab.

 

NOTE: If you have not done so already, install the mapr-spark package on your sandbox.  As the root user, run the following commands:

# vi /etc/yum.repos.d/mapr-eco.repo

--> change line containing baseurl to be equal to following:

baseurl=http://package.mapr.com/releases/MEP/MEP-2.0.0/redhat

 

Save the file and then run the following commands:

# yum remove mapr-spark

# yum clean all

# yum install mapr-spark

 

 

Verify that the  version installed is 2.0.1 so it agrees with the Kafka API we specified in pom.xml:

# rpm -qa | grep mapr-spark

 

The JSON producer record must conform to the following sample: 

{

"lastTimestamp":"2012-12-11",

"meanHigh":32.225999599999994,

"meanLow":31.783999799999997,

"meanOpen":32.0380006,

"meanClose":32.0719998,

"meanVolume":2.415158E7,

"lastClose":32.34

}

The syntax for running the Spark application is given below:

/opt/mapr/spark/spark-2.0.1/bin/spark-submit --class Lab2.StockSparkApp \

CS185-jar-with-dependencies.jar localhost:9092 local[2] prices stats mycg 5000

 

Standalone Kafka Consumer:

Implement the TODO's in the StockConsumer.java source code provided for this lab. The value of the "aggregated statistic" metric is calculated as follows:

 

meanVolume * (meanHigh + meanLow + meanOpen + meanClose) / 4.0

 

Then when calculating the delta percentage (difference between the previous aggregated statistic and the current one), you need to divide by the meanVolume, as shown below:

 

(currentAggregatedStatistic – previousAggregatedStatistic) / ( 100 * meanVolume)

 

You must consider positive, negative, and zero values above to formulate the right plan to buy, sell, or hold.

Your consumer must output to the screen a line for each batch of records it gets from the Kafka topic using the following format:

lastTimestamp,stockSymbol,lastClose,deltaPercentage,position

 

Here's a sample of output using 0.01 percent as the threshold:

2014-05-09,orcl,41.040001,-0.11007555956311095,buy

2014-05-16,orcl,41.689999,0.10516324601700763,sell

2014-05-23,orcl,42.150002,-0.14378334854710764,buy

2014-06-02,orcl,41.970001,0.004958062178341045,hold

2014-06-09,orcl,42.700001,-0.047328194260115676,buy

 

Note when the delta percentage is positive and greater than the threshold, we recommend "sell".  When delta percentage is negative and absolute value is greater than the threshold, we recommend "buy".

The syntax for running the standalone Java Kafka consumer is given below:

java -cp CS185-jar-with-dependencies.jar Lab2.StockConsumer localhost:9092 \

stats orcl mygroup 0.01

 

NOTE: you will need to change how the the main() method checks command-line argument count.  The code I provided has 4 but arg count is 5.  See the announcement on CANVAS on the same.

Submission:

Submit a ZIP file containing the 3 source files so that when I unzip the ZIP file, I see the following listed in the current working directory.  Do NOT create subdirectory structures in your ZIP file -- keep it flat.

StockProducer.java
StockSparkApp.java
StockConsumer.java
Grading

Your submission will be graded as follows:

8 points for correct records of key-value pairs in Kafka "prices" topic from the producer
8 points for correct records of key-value pairs in Kafka "stats" topic from the spark application
4 points for correct standard output from the consumer
 

Tips and hints 

1. build your pipeline gradually -- get the producer working before attempting the spark application, and get the spark application working before attempting the consumer.

2. you'll need to start the kafka services manually -- follow the instructions in the kafka-demo.txt file to start the broker and zookeeper.  

3. When you finally test your pipeline, make sure tha the producer, spark application, and consumer are all running at the same time.  That's how the graders will be running your code.

NOTE:If you run your producer, it will eventually exit after it's reached the end of the CSV file.  That's why we insert the Thread.sleep() in the producer -- to simulate a real-time production / consumption of the data.

4. if you are experiencing insufficient memory problems, then configure the apache kafka broker to use the already-running mapr zookeeper at localhost:5181 by setting the zookeeper.connect=localhost:5181 property in the config/server.properties file of your kafka distribution (and then restart the kafka server and kill your zookeeper server if it's running).

5. use System.out.println() and rdd.print() liberally to help debug your code, and don't forget you should test Kafka using kafka-console-consumer.sh to see what the actual records are being written to a given topic.